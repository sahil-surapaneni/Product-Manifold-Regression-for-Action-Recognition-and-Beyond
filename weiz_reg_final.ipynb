{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"weiz_reg_final.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO9PLpGjNvtYfJn/I3LLYPK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6FChF6FDq-LG"},"outputs":[],"source":["import numpy as np\n","from numpy import linalg as LA\n","import torch\n","from torch import linalg as LAT\n","import scipy\n","from scipy import linalg as LAS\n","from scipy.stats import unitary_group\n","import math\n","import cv2\n","import os\n","from scipy.stats import ortho_group\n","\n","from tensorly import unfold\n"]},{"cell_type":"code","source":["def import_data_weiz_reg(numFrames=35,height=50,width=50):\n","    \"\"\"\n","    Collects data from /data/ folder in current working directory, with folder names as labels.\n","    Inputs:\n","    -- numFrames=35\n","    -- height=50,\n","    -- width=50\n","    Outputs:\n","    -- Data as a numpy array (vid number,frame,height,width,color)\n","    -- Ordered list of video labels\n","    \"\"\"\n","    dataDir = os.getcwd()+\"/Weizmann rebuild/\"\n","    dataNameList = os.listdir(dataDir)\n","    fileCount = 0\n","    totalFiles = 0\n","    \n","    \n","        \n","    class_data = []\n","    labelList = []\n","    peopleOrder = []\n","    for foldername in os.listdir(dataDir):\n","        totalFiles = np.size(os.listdir(dataDir+foldername))\n","        bigBuffer = np.zeros((totalFiles,numFrames,height,width))\n","        label = []\n","        peopleOrder.append(foldername)\n","        fileCount = 0\n","        for filename in os.listdir(dataDir+foldername):\n","            \n","            vid = cv2.VideoCapture(dataDir+foldername+\"/\"+filename)\n","            vidFrames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n","            buffer = np.zeros((numFrames,height,width))\n","            \n","            for i in range(numFrames):\n","                sane, thisFrame = vid.read()                \n","                if not sane:\n","                    vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n","                    sane, thisFrame = vid.read()\n","                    if not sane:\n","                        print(\"something broke at\",i,filename)\n","                        break\n","                \n","                scale = 1\n","                height2, width2, channels = thisFrame.shape\n","                centerX,centerY=int(height2/2),int(width2/2)\n","                radiusX,radiusY = int(centerX*scale), int(centerY*scale)\n","                minX,maxX=centerX-radiusX,centerX+radiusX\n","                minY,maxY=centerY-radiusY,centerY+radiusY\n","                cropped = thisFrame[minX:maxX, minY:maxY]\n","                \n","                buffer[i] = cv2.cvtColor(cv2.resize(cropped,(width,height)),cv2.COLOR_BGR2GRAY)\n","                \n","            label.append(filename)\n","            bigBuffer[fileCount] = buffer\n","            fileCount += 1\n","            vid.release()\n","        class_data.append(bigBuffer)\n","        labelList.append(label)\n","        \n","    return class_data, labelList, peopleOrder\n","\n","data, labels, people = import_data_weiz_reg()"],"metadata":{"id":"DTn42py8rDXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import scipy\n","from tensorly import unfold\n","\n","leaveOut = 'subject1'\n","labelDict = {labels[0][i] : i for i in range(len(labels[0]))}\n","peopleDict = {people[i] : i for i in range(len(people))}\n","\n","leaveOutIndex = peopleDict[leaveOut]\n","def my_hosvd(A):\n","    dim = unfold(A,0)\n","    dim2 = unfold(A,1)\n","    dim3 = unfold(A,2)\n","    \n","    _,_,vh1 = LAS.svd(dim,full_matrices=False)\n","    _,_,vh2 = LAS.svd(dim2,full_matrices=False)\n","    _,_,vh3 = LAS.svd(dim3,full_matrices=False)\n","    \n","    return [np.matrix.getH(vh1),np.matrix.getH(vh2),np.matrix.getH(vh3)]\n","\n","def stieflel_to_grassman(stif):\n","    grass = []\n","    for i in range(len(stif)):\n","        grass.append(stif[i]@ortho_group.rvs(stif[i].shape[1]))\n","    return grass\n","\n","\n","\n","data_mls = []\n","for c in range(len(labels[0])):\n","    data_mls.append([])\n","    for f in range(3):\n","        data_mls[c].append([])\n","\n","for person in range(len(data)):\n","    if person == leaveOutIndex:\n","        continue\n","    for n in range(0,data[person].shape[0]):\n","        fac = my_hosvd(data[person][n])\n","        fac = stieflel_to_grassman(fac)\n","        classNum = labelDict[labels[person][n]]\n","        data_mls[classNum][0].append(fac[0])\n","        data_mls[classNum][1].append(fac[1])\n","        data_mls[classNum][2].append(fac[2])\n","\n","data_testing_set = []\n","for c in range(len(labels[0])):\n","    data_testing_set.append([])\n","    for f in range(3):\n","        data_testing_set[c].append([])\n","\n","for n in range(len(labels[leaveOutIndex])):\n","    factest = my_hosvd(data[leaveOutIndex][n])\n","    factest = stieflel_to_grassman(factest)\n","    classNum = labelDict[labels[leaveOutIndex][n]]\n","    data_testing_set[classNum][0].append(factest[0])\n","    data_testing_set[classNum][1].append(factest[1])\n","    data_testing_set[classNum][2].append(factest[2])\n"],"metadata":{"id":"uTDeAMujrIDA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def frob_norm_mat_calc(pset):\n","  size = len(pset)\n","  matrix = np.zeros((size,size))\n","  for i in range(size):\n","    for j in range(size):\n","      matrix[i,j] = np.power(np.linalg.norm(pset[i].T@pset[j],ord=\"fro\"),2)\n","  return matrix\n","\n","def build_pseudos(trainingData,listPseudos):\n","  numClasses = len(trainingData)\n","  for c in range(numClasses):\n","    for f in range(3):\n","      listPseudos[c][f].append(frob_norm_mat_calc(trainingData[c][f]))\n","\n","\n","def frob_norm_vec_calc(pset,x):\n","  size = len(pset)\n","  matrix = np.zeros((size,1))\n","  for i in range(size):\n","    matrix[i] = np.power(np.linalg.norm(x.T@pset[i],ord=\"fro\"),2)\n","  return matrix\n","\n","def error_factor(factorSet,x,subreg):\n","  numSample = len(factorSet)\n","  estimate = 0\n","  for i in range(numSample):\n","    estimate += (factorSet[i]@factorSet[i].T)*subreg[i]\n","  return np.linalg.norm(x@x.T-estimate,ord=\"fro\")\n","\n","def sub_lin_reg(factorSet,x,pseudo):\n","  #pseudo = frob_norm_mat_calc(factorSet)\n","  vector = frob_norm_vec_calc(factorSet,x)\n","  return 2*np.linalg.inv(pseudo + pseudo.T)@vector\n","\n","def lin_reg_class(pset,x,pseudos):\n","  numFactors = 3\n","  error = 0\n","  for f in range(numFactors):\n","    subreg = sub_lin_reg(pset[f],x[f],pseudos[f][0])\n","    error += error_factor(pset[f],x[f],subreg)\n","  return error\n","\n","def lin_reg_err(trainingData,x,pseudos):\n","  numClasses = len(trainingData)\n","  errors = np.zeros((numClasses,1))\n","  for c in range(numClasses):\n","    errors[c] = lin_reg_class(trainingData[c],x,pseudos[c])\n","  return errors\n","\n","pseudos = []\n","for c in range(len(data_mls)):\n","  pseudos.append([])\n","  for f in range(3):\n","    pseudos[c].append([])\n","build_pseudos(data_mls,pseudos)\n","\n","for i in range(len(labels[0])):\n","  pointForTest = [data_testing_set[i][0][0],data_testing_set[i][1][0],data_testing_set[i][2][0]]\n","  test = lin_reg_err(data_mls,pointForTest,pseudos)\n","  print(np.argmin(test))"],"metadata":{"id":"2grzpLmbrKv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import scipy\n","from tensorly import unfold\n","\n","confusion = np.zeros((len(labels[0]),len(labels[0])))\n","\n","for leaveOutPerson in range(len(people)):\n","\n","    leaveOutIndex = leaveOutPerson\n","\n","    data_mls = []\n","    for c in range(len(labels[0])):\n","        data_mls.append([])\n","        for f in range(3):\n","            data_mls[c].append([])\n","\n","    for person in range(len(data)):\n","        if person == leaveOutIndex:\n","            continue\n","        for n in range(0,data[person].shape[0]):\n","            fac = my_hosvd(data[person][n])\n","            fac = stieflel_to_grassman(fac)\n","            classNum = labelDict[labels[person][n]]\n","            data_mls[classNum][0].append(fac[0])\n","            data_mls[classNum][1].append(fac[1])\n","            data_mls[classNum][2].append(fac[2])\n","\n","    data_testing_set = []\n","    for c in range(len(labels[0])):\n","        data_testing_set.append([])\n","        for f in range(3):\n","            data_testing_set[c].append([])\n","\n","    for n in range(len(labels[leaveOutIndex])):\n","        factest = my_hosvd(data[leaveOutIndex][n])\n","        factest = stieflel_to_grassman(factest)\n","        classNum = labelDict[labels[leaveOutIndex][n]]\n","        data_testing_set[classNum][0].append(factest[0])\n","        data_testing_set[classNum][1].append(factest[1])\n","        data_testing_set[classNum][2].append(factest[2])\n","    pseudosList = []\n","    for c in range(len(data_mls)):\n","        pseudosList.append([])\n","        for f in range(3):\n","            pseudosList[c].append([])\n","    build_pseudos(data_mls,pseudosList)\n","    \n","    for i in range(len(labels[0])):\n","        pointForTest = [data_testing_set[i][0][0],data_testing_set[i][1][0],data_testing_set[i][2][0]]\n","        test = lin_reg_err(data_mls,pointForTest,pseudosList)\n","        confusion[i,np.argmin(test)] += 1\n"],"metadata":{"id":"3VB-uh2KrNFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.trace(confusion.T)\n","np.savetxt('confus_weiz_reg.csv',confusion,delimiter=',')"],"metadata":{"id":"y4BXGHOxrU2N"},"execution_count":null,"outputs":[]}]}